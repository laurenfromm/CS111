NAME: Lauren Fromm
EMAIL: laurenfromm@gmail.com
ID: 404751250


QUESTION 2.3.1 - Cycles in the basic list implementation:
Where do you believe most of the cycles are spent in the 1 and 2-thread list tests ?
Why do you believe these to be the most expensive parts of the code?
Where do you believe most of the time/cycles are being spent in the high-thread spin-lock tests?
Where do you believe most of the time/cycles are being spent in the high-thread mutex tests?

In the one and two thread list tests, most of the cycles are spent completing the operations of inserting, getting the list length, and deleting elements of the list. This is because when there aren’t many threads, the running process doesn’t have to worry about another thread holding the lock to a critical section, so it won’t have to wait for the lock to get opened. Thus, these parts are the most expensive parts of the code because only one thread can execute the critical sections at a time.
In the high-thread spin lock tests, most of the time is spent spinning while a thread waits for the lock to be free so that it can enter the critical section. When there are a lot of threads trying to access one section, most of the threads are spinning while only one is in a critical section. In the high-thread mutex tests, since the threads get put to sleep using system calls, most of the time is spent executing those system calls, which slows the program done.


QUESTION 2.3.2 - Execution Profiling:
Where (what lines of code) are consuming most of the cycles when the spin-lock version of the list exerciser is run with a large number of threads?
Why does this operation become so expensive with large numbers of threads?

The lines of code that consume the most cycles are the lines that try to get the lock, and then end up having to spin and wait for the lock to be open. This is especially bad for a large number of threads because when there are more threads, there are more processes trying to get the same lock, so  they almost all are spinning while only one is in the critical section.

QUESTION 2.3.3 - Mutex Wait Time:
Look at the average time per operation (vs. # threads) and the average wait-for-mutex time (vs. #threads).
Why does the average lock-wait time rise so dramatically with the number of contending threads?
Why does the completion time per operation rise (less dramatically) with the number of contending threads?
How is it possible for the wait time per operation to go up faster (or higher) than the completion time per operation?

As there are more threads, there are more processes trying to obtain one lock, so the threads end up spinning for longer periods of times as they try to get the lock, which is why the lock-wait time rise is so dramatic as there are more and more threads. It is less dramatic for overall time because it keeps track of the threads before and after they have finished going through the critical sections, so while there are threads waiting, there is still one thread getting work done. Since each process has its own clock, the wait time per operation goes up faster than the completion time because the total time is a global variable that is adding all of the seperate process' clocks, while the completion time uses one clock that starts before all of the threads and ends after all of the threads.

QUESTION 2.3.4 - Performance of Partitioned Lists
Explain the change in performance of the synchronized methods as a function of the number of lists.
Should the throughput continue increasing as the number of lists is further increased? If not, explain why not.
It seems reasonable to suggest the throughput of an N-way partitioned list should be equivalent to the throughput of a single list with fewer (1/N) threads. Does this appear to be true in the above curves? If not, explain why not.

As there are more lists, the total time per operation goes down which means there is higher throughput overall. The throughput will increase up to a certain point when the number of lists increase. When there are the same amount of lists as the amount of threads then each thread is operating on one list so the throughput could not get better. It might seem reasonable to think that an N-way partitioned list should be equivilant to the throughput of a single list with fewer (1/N) threads, but the graphs don't show this trend perfectly.


File included:
SortedList.h: Header program declaring linked list functions

SortedList.c: Program defining linked list functions such as adding to a linked list, deleting, finding items, and finding the length of the list.

lab2_list.c: The source code to implement one or more threads to do operations on a linked list and report performance, allowing multiple sublists, yield operations, and synchronization options that can also be added to compilation.

Makefile: Builds the programs, runs multiple tests, graphs the output, makes the tarbell, and cleans the files.

README: File including answers to questions given and description of files included.

lab2b_list.csv: File containing the results of all the tests ran.

profile.out: File containing profiling report from the spin lock implementaion.

lab2b_1.png: Graph of throughput versus threads for both synchronized option

lab2b_2.png: Shows time per mutex wait and time per operation for mutex operations.

lab2b_3.png: Graph of successful iterations versus threads for each synchronization method

lab2b_4.png: Graph of throughput versus number of threads for mutex method.

lab2b_5.png: Graph of throughput versus number of threads for spin lock method.

lab2b_list.gp: Graph script to make the graphs above.